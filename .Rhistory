library(tidymodels)
library(tidyverse)
library(vroom)
library(embed)
library(baguette)
library(bonsai)
library(dbarts)
test <- vroom("test.csv")
train <- vroom("train.csv")
my_recipe <- recipe(loss ~ ., data = train) %>%
update_role(id, new_role = "id") %>%
step_zv(all_predictors()) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(loss)) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data=train)
bart_model <- bart(trees= 500,
prior_terminal_node_coef = .95,
prior_terminal_node_expo = 2) %>%
set_engine("dbarts") %>%
set_mode("regression")
detach("package:dbarts", unload = TRUE)
bart_model <- bart(trees= 500,
prior_terminal_node_coef = .95,
prior_terminal_node_expo = 2) %>%
set_engine("dbarts") %>%
set_mode("regression")
bart_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(bart_model) %>%
fit(data = train)
View(test)
bart_preds <- predict(bart_wf, new_data = test)
View(bart_preds)
kaggle_submission <- bart_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(id, .pred) %>% #Just keep datetime and prediction variables
rename(loss=.pred)
View(kaggle_submission)
## Write out the file
vroom_write(x=kaggle_submission, file="./Bartmodel.csv", delim=",")
